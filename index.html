<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name=viewport content="width=device-width, initial-scale=1">
<meta name="google-site-verification" content="google4c0a25b43370ae9c.html" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="icon" href="photo_kaiqing.png" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Kaiqing Zhang's Homepage</title> 
</head> 
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top"> 
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="experience.html">Experience</a></div>
<div class="menu-item"><a href="misc.html">Misc.</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Kaiqing Zhang</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://kzhang66.github.io/"><img src="photo_kaiqing.png" alt="alt text" width="185px" height="250px" /></a>&nbsp;</td>
<td align="left">
<p> <b>Ph.D. Candidate</b></p>  
<a href="https://ece.illinois.edu/">Department of Electrical and Computer Engineering (ECE)</a> <br />
<a href="https://csl.illinois.edu/">Coordinated Science Laboratory (CSL)</a> <br />
<a href="https://www.illinois.edu">University of Illinois at Urbana-Champaign (UIUC)</a> </p>

Office: Room 360, Coordinated Science Laboratory <br />
Address: 1308 W Main St, Urbana, IL, 61801 <br /> 
Tel: 217-979-1869 <br />  
E-mail: kzhang66@illinois.edu </p>
  
<p><a href="My_CV.pdf">CV</a><br />
<a href="https://scholar.google.com/citations?hl=en&user=Hi7ZdhQAAAAJ&sortby=pubdate&view_op=list_works&gmla=AJsN-F6J8zj9uxG4y2WQ9nkE5sd25QY7XEP0BqGZbLQNyhK-tewCB_-d8oaGeQ7rXg1J4T5ityD9cXZBtVt46c-c_oJndAKkXEjAciIoZnPaBw3vFvFutgk">Google Scholar</a><br />	
<!-- <a href="https://www.researchgate.net/profile/Kaiqing_Zhang">ResearchGate</a></p>     -->
</td></tr></table> 
<p>  </p> 
<h2>About Me</h2>
<p>I am a Ph.D. Candidate in the Department of Electrical and Computer Engineering, and affiliated with the Coordinated Science Laboratory at 
the <a href="https://www.illinois.edu">University of Illinois at Urbana-Champaign</a>. I am fortunate to be advised by <a href="http://tamerbasar.csl.illinois.edu/">Prof. Tamer Başar</a>. My research interests lie broadly in control theory, game theory, reinforcement learning, and their intersections; with applications in intelligent and distributed multi-agent systems, including smart grid, robotics, and transportation systems.
<!-- I have also been fortunate to work with <a href="http://sites.utexas.edu/haozhu/">Prof. Hao Zhu</a> (2015-2017) for my Master's thesis.</p>  -->
<!-- <p>My research interests lie broadly in control theory, game theory, reinforcement learning, and their intersections; with applications in intelligent and distributed multi-agent systems, including smart grid, robotics, and transportation systems.</p> -->
<h2>Education</h2>
<ul>
<li><p> University of Illinois at Urbana-Champaign, Urbana, IL: Ph.D. Candidate, <b>Electrical and Computer Engineering</b>, 2017 - Present</p>
<li> <p> University of Illinois at Urbana-Champaign, Urbana, IL: M.S., <b>Applied Mathematics</b>, 2015 - 2017</p>
<li> <p> University of Illinois at Urbana-Champaign, Urbana, IL: M.S., <b>Electrical and Computer Engineering</b>, 2015 - 2017</p>
<li><p> Tsinghua University, Beijing, China: B.S. in <b>Automation</b> (with Honor) & Dual Degree in <b>Economics</b>, 2011 - 2015</p>
</li>
</ul>		
<h2>Recent News</h2>
<h4>I am co-organizing the online seminar series <i><b> <a href="https://sites.google.com/view/gamesdecisionsnetworks">Games, Decisions & Networks</a></b></i>, welcome to <a href="https://groups.google.com/u/2/g/games-decisions-networks-seminar">join us</a>!</h4>
<ul>
<li><p>May 2021: Our <b><a href="https://arxiv.org/pdf/1910.09496.pdf">paper</a></b> on <b>policy optimization for robust  control</b></a></b> accepted to SIAM Journal on Control and Optimization (SICON).</p>
<li><p>May 2021: Two papers accepted to <b>ICML</b> 2021.</p>	
<li><p>Feb. 2021: I am honored to be awarded the <b>Simons-Berkeley Research Fellowship</b> from Simons Institute at Berkeley, for the program <b><a href="https://simons.berkeley.edu/programs/games2022">Learning and Games</a></b>.</p>
<li><p>Feb. 2021: Update our <b><a href="https://arxiv.org/pdf/1910.09496.pdf">paper</a></b> on <b>RL for robust  control</b>, by strengthening the global convergence and implicit regularization results. Also, excited to see that our policy optimization methods  can be  numerically <b>much  faster</b> than some existing general robust control solvers, with competitive H2 and Hinf performance being achieved.</p>	
<li><p>Jan. 2021: New <b> <a href="https://arxiv.org/pdf/2101.01041.pdf">paper</a></b> is posted on arXiv, which develops sample-based  policy optimization methods for <b>risk-sensitive/robust control</b> problems, with <b>global convergence</b> and <b>sample complexity</b> guarantees. It also solves LQ zero-sum dynamic games, a benchmark setting of <b>multi-agent RL</b>, in a <b>model-free</b> way.</p>	
<li><p>Jan. 2021: New <b> <a href="https://openreview.net/pdf?id=P6_q1BRxY8Q">paper</a></b> accepted to <a href="https://iclr.cc/"><b>ICLR</b> 2021</a>, on <b>scalable (~1000 agents) and safe</b> multi-agent control by learning decentralized control barrier functions.</p>
<li><p>Aug.-Dec. 2020: I visit Simons Institute at Berkeley for the program <b> <a href="https://simons.berkeley.edu/programs/rl20">Theory of Reinforcement Learning</a></b> this Fall.</p>	
<li><p>Sept. 2020: Papers accepted to <b>NeurIPS</b> 2020, with one <b><a href="https://arxiv.org/pdf/2007.07461.pdf">Spotlight</a></b>.</p>	
<!-- <li><p>July 2020: New paper <a href="https://arxiv.org/pdf/2007.07461.pdf">Model-based multi-agent RL in zero-sum Markov games with near-optimal sample complexity</a> has been posted on arXiv. This paper shows that this simple approach achieves <b>near-minimax optimal</b> sample complexity in the most basic MARL setting. More interestingly, we show a separation of sample complexity that is unique in MARL, demonstrating both the <b>power</b> and <b>limitation</b> of this model-based approach.</p> -->
<!-- <li><p>July 2020: New paper <a href="https://arxiv.org/pdf/2006.04672.pdf">POLY-HOOT: Monte-Carlo planning in continuous space MDPs with non-asymptotic analysis</a> has been posted on arXiv. This paper provides a non-asymptotic analysis on MCTS solving continuous space MDPs, motivated from control applications.</p>	 -->
<!-- <li><p>July 2020: Two papers on <a href="https://arxiv.org/pdf/2004.01098.pdf">partially observable collaborative MARL</a> and <a href="https://miehling.csl.illinois.edu/files/zzmb-cdc2020-extended.pdf">MARL in mean-field games</a> have been accepted to <b>IEEE Conf. on Decision and Control (CDC)</b>, Jeju-do, South Korea, 2020.</p> -->
<!-- <li><p>June 2020: I passed my Preliminary Exam! Heading to the last stage of my Ph.D. Thanks a lot for the support and feedback from <a href="http://tamerbasar.csl.illinois.edu/">Prof. Başar</a>, <a href="https://sites.google.com/a/illinois.edu/srikant/">Prof. Srikant</a>, <a href="http://maxim.ece.illinois.edu/">Prof. Raginsky</a>, <a href="https://ece.illinois.edu/about/directory/faculty/dullerud">Prof. Dullerud</a>, and <a href="https://binhu7.github.io/">Prof. Hu</a>!</p> -->
<li><p>Mar. 2020: I am awarded the prestigious <b> <a href="https://ece.illinois.edu/academics/grad/fellowships/hong">Hong, McCully, and Allen Fellowship</a> ($12000)</b> (highest fellowship in ECE Department).</p>
<li><p>Mar. 2020: Shorter version of our paper <a href="https://arxiv.org/pdf/1910.09496.pdf">Policy optimization for H-2 linear control with H-infinity robustness guarantee: Implicit regularization and global convergence</a> has been accepted to <b><a href="https://sites.google.com/berkeley.edu/l4dc/home">L4DC</a></b>, and selected as one of the 14 top papers for <b>Oral Presentation</b>.</p>
<!-- <li><p>Mar. 2020: I am awarded the <b>YEE Fellowship</b> from the College of Engineering for the year 2020-2021!</p> -->
<li><p>Feb. 2020: I present at the <b><a href="https://www.ipam.ucla.edu/programs/workshops/intersections-between-control-learning-and-optimization/?tab=overview">Intersections between Control, Learning and Optimization Workshop</a></b> held by Institute for Pure & Applied Math. (IPAM) at UCLA.</p>
<!-- <li><p>Feb. 2020: Two papers related to MARL have been accepted to <b>IFAC World Congress</b>, Berlin, Germany, 2020.</p>	 -->
<!-- <li><p>Feb. 2020: Our paper ''Approximate equilibrium computation for discrete-time linear-quadratic mean-field games'' has been accepted to <b>American Control Conf. (ACC)</b>, Denver, CO, 2020.</p> -->
<!-- <li><p>Jan. 2020: I am invited to give talks on reinforcement learning at the <a href="https://ee-ciss.princeton.edu/"> Conference on Information Sciences and Systems <b>(CISS)</b></a> 2020 at Princeton!</p> -->
<!-- <li><p>Dec. 2019: I am presenting at <b>NeurIPS</b> 2019.</p> -->
<li><p>Nov. 2019: Invited Chapter <a href="https://arxiv.org/pdf/1911.10635.pdf">Multi-Agent Reinforcement Learning:
A Selective Overview of Theories and Algorithms</a> to the <i>Springer Studies in Systems, Decision and Control, Handbook on RL and Control</i>  has been posted on arXiv. This paper focuses on reviewing multi-agent RL algorithms that are backed by theoretical analysis.</p>
<!-- <li><p>Oct. 2019: New paper <a href="https://arxiv.org/pdf/1910.09496.pdf">Policy optimization for H-2 linear control with H-infinity robustness guarantee: Implicit regularization and global convergence</a> has been posted on arXiv. This paper studies the <b>implicit regularization</b> property of policy-based RL  methods for robust control design.</p> -->
<!-- <li><p>Sept. 2019: Our paper accepted to <b>NeurIPS</b> 2019: <a href="https://arxiv.org/pdf/1906.00729.pdf">Policy optimization provably converges to Nash equilibria in zero-sum linear quadratic games</a>. This paper studies the global convergence of policy optimization algorithms for zero-sum linear quadratic games, a fundamental setting of multi-agent RL.</p> -->
<!-- <li><p>Sept. 2019: Our paper accepted to <b>NeurIPS</b> 2019: <a href="https://arxiv.org/pdf/1911.04220.pdf">Non-cooperative inverse reinforcement learning</a>. This paper provides a new formalism for non-cooperative inverse RL, by modeling it as a zero-sum -->
<!-- Markov game with one-sided incomplete information.</p> -->
<!-- </div> -->
</li>
</ul>

<div id="footer">
<div id="footer-text">
Page generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
<!-- Default Statcounter code for Kaiqing's Website https://kzhang66.github.io/
-->
<script type="text/javascript">
var sc_project=12155422; 
var sc_invisible=1; 
var sc_security="73574776"; 
var sc_https=1; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js" async></script>
<noscript><div class="statcounter"><a title="Web Analytics Made Easy -
StatCounter" href="https://statcounter.com/" target="_blank"><img
class="statcounter" src="https://c.statcounter.com/12155422/0/73574776/1/"
alt="Web Analytics Made Easy - StatCounter"></a></div></noscript>
<!-- End of Statcounter Code -->
</table>
</body>
</html>

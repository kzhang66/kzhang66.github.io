<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name=viewport content="width=device-width, initial-scale=1">
<meta name="google-site-verification" content="google4c0a25b43370ae9c.html" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="icon" href="photo_kaiqing_2.JPG" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Kaiqing Zhang's Homepage</title> 
</head> 
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top"> 
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="misc.html">Misc.</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Kaiqing Zhang</h1>
</div>
<table class="imgtable"><tr><td>
<!-- <a href="https://kzhang66.github.io/"><img src="photo_kaiqing_2.JPG" alt="alt text" width="251px" height="250px" /></a>&nbsp;</td> -->
<a href="https://kzhang66.github.io/"><img src="photo_kaiqing_2.JPG" alt="alt text" width="241px" height="240px" /></a>&nbsp;</td>
<td align="left">
<p> <b>Assistant Professor</b></p>  
<!-- <a href="https://www.eecs.mit.edu/">Department of Electrical Engineering and Computer Science (EECS)</a> <br /> -->
<a href="https://ece.umd.edu/">Electrical and Computer Engineering (ECE)</a> <br />
<a href="https://www.cs.umd.edu/">Computer Science (CS)</a> <br />
<a href="https://isr.umd.edu/">Institute for Systems Research (ISR)</a> <br />
<a href="https://ml.umd.edu/">Center for Machine Learning</a> <br />
<a href="https://www.umiacs.umd.edu/">University of Maryland Institute for Advanced Computer Studies (UMIACS)</a> <br />
<a href="https://robotics.umd.edu/">Maryland Robotics Center (MRC)</a> <br />
<a href="https://www.umd.edu">University of Maryland, College Park</a> </p>

<!-- Office: Room 2117, Coordinated Science Laboratory <br /> -->
<!-- Address: 1308 W Main St, Urbana, IL, 61801 <br />  -->
<!-- E-mail: kaiqing@{umd,mit,csail.mit}.edu (first one preferred)</p> -->
<!-- <p> E-mail: kaiqing@umd.edu</p> -->
  
<p><a href="Kaiqing_CV.pdf">CV</a><br />
<a href="https://scholar.google.com/citations?hl=en&user=Hi7ZdhQAAAAJ&sortby=pubdate&view_op=list_works&gmla=AJsN-F6J8zj9uxG4y2WQ9nkE5sd25QY7XEP0BqGZbLQNyhK-tewCB_-d8oaGeQ7rXg1J4T5ityD9cXZBtVt46c-c_oJndAKkXEjAciIoZnPaBw3vFvFutgk">Google Scholar</a><br />	
<!-- <a href="https://twitter.com/KaiqingZhang">Twitter</a><br />	 -->
<!-- <a href="https://www.researchgate.net/profile/Kaiqing_Zhang">ResearchGate</a></p>     -->
</td></tr></table> 
<p>  </p> 
<h2>About Me</h2>
<p>I am an Assistant Professor at the Department of <a href="https://ece.umd.edu/">Electrical and Computer Engineering (ECE)</a> and the <a href="https://isr.umd.edu/">Institute for Systems Research (ISR)</a>, at the <a href="https://ece.umd.edu/clark/faculty/1748/Kaiqing-Zhang">University of Maryland, College Park</a>. I am also affiliated with the Department of <a href="https://www.cs.umd.edu/">Computer Science (CS)</a>, the <a href="https://www.umiacs.umd.edu/">University of Maryland Institute for Advanced Computer Studies (UMIACS)</a>, the <a href="https://ml.umd.edu/">Center for Machine Learning</a>, and the <a href="https://robotics.umd.edu/">Maryland Robotics Center (MRC)</a>. During the deferral time before joining Maryland, I was a postdoctoral scholar affiliated with <a href="https://lids.mit.edu/">LIDS</a> and <a href="https://www.csail.mit.edu/">CSAIL</a>  at MIT, and a Research Fellow at <a href="https://simons.berkeley.edu/">Simons Institute for the Theory of Computing</a> at Berkeley. I finished my Ph.D. from the Department of <a href="https://ece.illinois.edu/">ECE</a> and <a href="https://csl.illinois.edu/">CSL</a> at the <a href="https://www.illinois.edu">University of Illinois at Urbana-Champaign (UIUC)</a>. I also received M.S. in both ECE and Applied Math from UIUC, and B.E. from <a href="https://www.tsinghua.edu.cn/en/index.htm">Tsinghua University</a>. My research interests lie broadly in Control and Decision Theory, Game Theory, Robotics, Reinforcement/Machine Learning, Computation, and their intersections. 

<h2>Openings</h2> 
<p>I am actively looking for <b>self-motivated Ph.D. students</b> and <b>postdocs</b> with a <b>strong mathematical and/or programming</b> background. I am also happy to host (remote) undergraduate/graduate visitors. 
<!-- Due to the volume of emails that I receive inquiring openings (which we do have),  -->
I might be slow in responding to unsolicited emails regarding openings. Nevertheless, if you have a strong background and think your interests are compatible with mine, <b>please do not hesitate to contact me with your CV</b>. Also feel free to contact me if you are already admitted and looking for an advisor, or mention my name in your Ph.D. application (in either ECE or CS) if you are interested in working with me.</p>
<!-- <ul> -->

<h2>Recent News</h2>
<!-- <h4>I am co-organizing the online seminar series <i><b> <a href="https://sites.google.com/view/gamesdecisionsnetworks">Games, Decisions & Networks</a></b></i>, welcome to <a href="https://groups.google.com/u/2/g/games-decisions-networks-seminar">join us</a>!</h4> -->
<ul>
<li><p>Dec. 2023: Invited to speak at the 	<a href="https://ee-ciss.princeton.edu/">Annual Conference on Information Sciences and Systems (CISS)</a>'s Modern Reinforcement Learning Session.
<li><p>Dec. 2023: Invited to speak at the 	<a href="https://ios2024.rice.edu/">INFORMS Optimization Society (IOS) Conference</a>'s Recent Advances in Min-Max Optimization and Beyond Session.	
<li><p>Dec. 2023: Invited to give a tutorial at the ETH/EPFL Multi-Agent Reinforcement Learning Summer School in 2024.
<li><p>Sept. 2023: Invited to speak at the 	<a href="https://allerton.csl.illinois.edu/">Allerton Conference</a> 2023 on the <a href="https://arxiv.org/pdf/2308.08705.pdf">work</a>.
<li><p>Sept. 2023: We have 4/5 submitted papers accepted to NeurIPS 2023.
<!-- <li><p>June 2023: Attended and spoke at <a href="https://acc2023.a2c2.org/workshops/">AsuFest</a>.  -->
<li><p>June 2023: Attended and gave an invited tutorial at <a href="https://l4dc.seas.upenn.edu">5th Annual Learning for Dynamics & Control (L4DC) Conference</a>. 
<li><p>May 2023: Organized a <a href="https://acc2023.a2c2.org/workshops/">workshop</a> at American Control Conference (ACC) 2023, and invited to speak at <a href="https://www.siam.org/conferences/cm/program/program-and-abstracts/op23-program-abstracts">SIAM Optimization (OP) 23</a>, on policy optimization for control. 
<li><p>May 2023: Three papers accepted to COLT 2023, <a href="https://arxiv.org/pdf/2204.03991.pdf">one</a> on the complexity of solving general-sum stochastic games; <a href="https://arxiv.org/pdf/2307.06457.pdf">one</a> on out-of-support distribution shift; and <a href="https://arxiv.org/pdf/2302.03673.pdf">one</a> on independent function approximation in multi-agent RL. 
<li><p>April 2023: Our <a href="https://arxiv.org/pdf/2212.14511.pdf">paper</a> on direct latent model learning for LQG control has been selected as Oral at <a href="https://l4dc.seas.upenn.edu">L4DC 2023</a>. 
<li><p>April 2023: Two papers accepted to ICML 2023, <a href="https://arxiv.org/pdf/2212.13861.pdf">one</a> on offline RL with general function approximation, <a href="https://arxiv.org/pdf/2308.08705.pdf">one</a> on partially observable multi-agent RL (first submission and acceptance with my student, congrats Xiangyu!). 
<!-- <li><p>March 2023: New  <a href="https://arxiv.org/pdf/2302.03673.pdf">paper</a> on independent function approximation for decentralized multi-agent RL, and <a href="https://arxiv.org/pdf/2303.03100.pdf">paper</a> on finite-sample analysis of payoff-based learning in zero-sum stochastic games.</p>	 -->
<!-- <li><p>March 2023: Invited to give a tutorial (with several fantastic colleagues) at the <a href="https://l4dc.seas.upenn.edu">5th Annual Learning for Dynamics & Control (L4DC) Conference</a> in June. 
 -->
<li><p>Feb. 2023: Invited to speak at the 	<a href="https://ita.ucsd.edu/">Information Theory and Applications (ITA) Workshop</a>'s Learning and Control Session. 	
<li><p>Jan. 2023: Honored to be invited to speak at the 	<a href="https://meyn.ece.ufl.edu/c3/6th-workshop-on-cognition-control/">Cognition and Control in Complex Systems Workshop</a> hosted by Prof. Sean Meyn. 
<li><p>Jan. 2023: Our <a href="https://arxiv.org/pdf/2210.10947.pdf">paper</a> on decentralized self-supervised learning is accepted to ICLR 2023, which shows the interesting and unique benefit of using self-supervised learning in decentralized/federated learning.
<li><p>Jan. 2023: We have 2/2 submitted papers accepted to AISTATS 2023, and 3/4 submitted papers accepted to ICLR 2023.
<!-- <li><p>Dec. 2022: New  <a href="https://arxiv.org/pdf/2212.14511.pdf">paper</a> on direct latent model learning for LQG control, and <a href="https://arxiv.org/pdf/2212.13861.pdf">paper</a> on offline reinforcement learning with general function approximation.</p> -->
<li><p>Oct. 2022: We put together an <a href="https://arxiv.org/pdf/2210.04810.pdf">invited article</a> for <a href="https://www.annualreviews.org/journal/control">Annual Review of Control, Robotics, and Autonomous Systems</a>, on policy optimization for learning control policies.</p>
<li><p>Oct. 2022: I am awarded the <a href="https://publish.illinois.edu/phd-thesis-award/">CSL Ph.D. Thesis Award</a> for contributions in reinforcement learning, control theory, and game theory.</p>
<li><p>Sept. 2022: Our <a href="https://arxiv.org/pdf/2202.11659.pdf">paper</a> on provable policy gradient methods for output estimation problems in control; and <a href="https://arxiv.org/pdf/2206.04502.pdf">paper</a> on the generalization guarantees of minimax optimization in machine learning,  are accepted to NeurIPS 2022. The latter was  also accepted as Oral (4 out of all submissions) in the <i>New Frontiers in Adversarial Machine Learning Workshop</i>, ICML 2022.</p>	
<li><p>July 2022: Our <a href="https://proceedings.mlr.press/v162/suh22b/suh22b.pdf">paper</a> on differentiable simulator for robotics has been selected as Outstanding Paper Award for ICML 2022.</p>
<li><p>June 2022: <a href="https://arxiv.org/pdf/2111.11743.pdf">Invited article</a> at International Congress of Mathematicians (ICM), companioned by Asu's talk at <a href="https://www.youtube.com/watch?v=B4RZw9kDXKc&ab_channel=InternationalMathematicalUnion">ICM</a> 2022 and <a href="https://iccopt2022.lehigh.edu/scientific-program/plenary-speakers/">ICCOPT</a> 2022 Plenary.</p>	
<li><p>May 2022: Our <a href="https://arxiv.org/pdf/2202.04129.pdf">paper</a> on independent policy optimization for multi-agent RL (Long Oral); <a href="https://proceedings.mlr.press/v162/mao22a/mao22a.pdf">paper</a> on improving the exploration efficiency in multi-agent RL (Short Oral); and <a href="https://proceedings.mlr.press/v162/suh22b/suh22b.pdf">paper</a> on differentiable simulator for robotics (Long Oral),  are accepted to ICML 2022.</p>
<li><p>May 2022: Our <a href="https://arxiv.org/pdf/2205.11389.pdf">paper</a> on fictitious play in identical-interest stochastic games, is accepted to EC 2022.</p> 
<li><p>Jan.-May 2022: I visit Simons Institute at Berkeley as a Research Fellow for the program <a href="https://simons.berkeley.edu/programs/games2022">Learning and Games</a> this spring.</p>		
<li><p>Sept. 2021: Our <a href="https://arxiv.org/pdf/2106.02748.pdf">paper</a> on decentralized Q-learning for zero-sum stochastic games in multi-agent RL; and <a href="https://arxiv.org/pdf/2101.01041.pdf">paper</a> on the sample complexity of policy gradient methods for solving risk-sensitive and robust control, are accepted to NeurIPS 2021. 
<!-- , which develops <b>decentralized</b> Q-learning for <b>zero-sum stochastic games</b>, with provable convergence guarantees, and minimal information requirement to the agent. The learning dynamics is <b>rational</b>, <b>convergent</b>, and <b>radically uncoupled</b>.</p>		 -->
<li><p>May 2021: I defend my Ph.D., and will be joining the University of Maryland, College Park as an Assistant Professor in Oct. 2022. I also start my Postdoc at MIT. 
<!-- , and will be joining the <b>Department of Electrical and Computer Engineering (ECE)</b> and <b>Institute for Systems Research (ISR)</b> at the <b>University of Maryland, College Park</b> as an Assistant Professor in Oct., 2022! Please feel free to reach out for potential opportunities/collaborations.</p>	 -->
<li><p>May 2021: Our <a href="https://arxiv.org/pdf/1910.09496.pdf">paper</a> on policy optimization for robust  control</a> is accepted to SIAM Journal on Control and Optimization (SICON).</p>
<!-- <li><p>May 2021: Two papers are accepted to ICML 2021.</p>	 -->
<!-- <li><p>Feb. 2021: I am honored to be awarded the <b>Simons-Berkeley Research Fellowship</b> from Simons Institute at Berkeley, for the program <b><a href="https://simons.berkeley.edu/programs/games2022">Learning and Games</a></b>.</p> -->
<!-- <li><p>Feb. 2021: Update our <b><a href="https://arxiv.org/pdf/1910.09496.pdf">paper</a></b> on <b>RL for robust  control</b>, by strengthening the global convergence and implicit regularization results. Also, excited to see that our policy optimization methods  can be  numerically <b>much  faster</b> than some existing general robust control solvers, with competitive H2 and Hinf performance being achieved.</p>	 -->
<!-- <li><p>Jan. 2021: New <b> <a href="https://arxiv.org/pdf/2101.01041.pdf">paper</a></b> is posted on arXiv, which develops sample-based  policy optimization methods for <b>risk-sensitive/robust control</b> problems, with <b>global convergence</b> and <b>sample complexity</b> guarantees. It also solves LQ zero-sum dynamic games, a benchmark setting of <b>multi-agent RL</b>, in a <b>model-free</b> way.</p>	 -->
<li><p>Jan. 2021: Our <a href="https://openreview.net/pdf?id=P6_q1BRxY8Q">paper</a> on scalable (~1000 agents) and safe multi-agent control by learning decentralized control barrier functions, is accepted to ICLR 2021.</p>
<!-- <li><p>Aug.-Dec. 2020: I visit Simons Institute at Berkeley for the program <b> <a href="https://simons.berkeley.edu/programs/rl20">Theory of Reinforcement Learning</a></b> this Fall.</p>	 -->
<li><p>Sept. 2020: Papers accepted to NeurIPS 2020, with one <a href="https://arxiv.org/pdf/2007.07461.pdf">Spotlight</a>.</p>	
<!-- <li><p>July 2020: New paper <a href="https://arxiv.org/pdf/2007.07461.pdf">Model-based multi-agent RL in zero-sum Markov games with near-optimal sample complexity</a> has been posted on arXiv. This paper shows that this simple approach achieves <b>near-minimax optimal</b> sample complexity in the most basic MARL setting. More interestingly, we show a separation of sample complexity that is unique in MARL, demonstrating both the <b>power</b> and <b>limitation</b> of this model-based approach.</p> -->
<!-- <li><p>July 2020: New paper <a href="https://arxiv.org/pdf/2006.04672.pdf">POLY-HOOT: Monte-Carlo planning in continuous space MDPs with non-asymptotic analysis</a> has been posted on arXiv. This paper provides a non-asymptotic analysis on MCTS solving continuous space MDPs, motivated from control applications.</p>	 -->
<!-- <li><p>July 2020: Two papers on <a href="https://arxiv.org/pdf/2004.01098.pdf">partially observable collaborative MARL</a> and <a href="https://miehling.csl.illinois.edu/files/zzmb-cdc2020-extended.pdf">MARL in mean-field games</a> have been accepted to <b>IEEE Conf. on Decision and Control (CDC)</b>, Jeju-do, South Korea, 2020.</p> -->
<!-- <li><p>June 2020: I passed my Preliminary Exam! Heading to the last stage of my Ph.D. Thanks a lot for the support and feedback from <a href="http://tamerbasar.csl.illinois.edu/">Prof. Başar</a>, <a href="https://sites.google.com/a/illinois.edu/srikant/">Prof. Srikant</a>, <a href="http://maxim.ece.illinois.edu/">Prof. Raginsky</a>, <a href="https://ece.illinois.edu/about/directory/faculty/dullerud">Prof. Dullerud</a>, and <a href="https://binhu7.github.io/">Prof. Hu</a>!</p> -->
<!-- <li><p>Mar. 2020: I am awarded the prestigious <b> <a href="https://ece.illinois.edu/academics/grad/fellowships/hong">Hong, McCully, and Allen Fellowship</a> ($12000)</b> (highest fellowship in ECE Department).</p> -->
<!-- <li><p>Mar. 2020: Shorter version of our paper <a href="https://arxiv.org/pdf/1910.09496.pdf">Policy optimization for H-2 linear control with H-infinity robustness guarantee: Implicit regularization and global convergence</a> has been accepted to <b><a href="https://sites.google.com/berkeley.edu/l4dc/home">L4DC</a></b>, and selected as one of the 14 top papers for <b>Oral Presentation</b>.</p> -->
<!-- <li><p>Mar. 2020: I am awarded the <b>YEE Fellowship</b> from the College of Engineering for the year 2020-2021!</p> -->
<!-- <li><p>Feb. 2020: I present at the <b><a href="https://www.ipam.ucla.edu/programs/workshops/intersections-between-control-learning-and-optimization/?tab=overview">Intersections between Control, Learning and Optimization Workshop</a></b> held by Institute for Pure & Applied Math. (IPAM) at UCLA.</p> -->
<!-- <li><p>Feb. 2020: Two papers related to MARL have been accepted to <b>IFAC World Congress</b>, Berlin, Germany, 2020.</p>	 -->
<!-- <li><p>Feb. 2020: Our paper ''Approximate equilibrium computation for discrete-time linear-quadratic mean-field games'' has been accepted to <b>American Control Conf. (ACC)</b>, Denver, CO, 2020.</p> -->
<!-- <li><p>Jan. 2020: I am invited to give talks on reinforcement learning at the <a href="https://ee-ciss.princeton.edu/"> Conference on Information Sciences and Systems <b>(CISS)</b></a> 2020 at Princeton!</p> -->
<!-- <li><p>Dec. 2019: I am presenting at <b>NeurIPS</b> 2019.</p> -->
<!-- <li><p>Nov. 2019: Invited Chapter <a href="https://arxiv.org/pdf/1911.10635.pdf">Multi-Agent Reinforcement Learning: -->
<!-- A Selective Overview of Theories and Algorithms</a> to the <i>Springer Studies in Systems, Decision and Control, Handbook on RL and Control</i>  has been posted on arXiv. This paper focuses on reviewing multi-agent RL algorithms that are backed by theoretical analysis.</p> -->
<!-- <li><p>Oct. 2019: New paper <a href="https://arxiv.org/pdf/1910.09496.pdf">Policy optimization for H-2 linear control with H-infinity robustness guarantee: Implicit regularization and global convergence</a> has been posted on arXiv. This paper studies the <b>implicit regularization</b> property of policy-based RL  methods for robust control design.</p> -->
<!-- <li><p>Sept. 2019: Our paper accepted to <b>NeurIPS</b> 2019: <a href="https://arxiv.org/pdf/1906.00729.pdf">Policy optimization provably converges to Nash equilibria in zero-sum linear quadratic games</a>. This paper studies the global convergence of policy optimization algorithms for zero-sum linear quadratic games, a fundamental setting of multi-agent RL.</p> -->
<!-- <li><p>Sept. 2019: Our paper accepted to <b>NeurIPS</b> 2019: <a href="https://arxiv.org/pdf/1911.04220.pdf">Non-cooperative inverse reinforcement learning</a>. This paper provides a new formalism for non-cooperative inverse RL, by modeling it as a zero-sum -->
<!-- Markov game with one-sided incomplete information.</p> -->
<!-- </div> -->
</li>
</ul>

<div id="footer">
<div id="footer-text">
Page generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
<!-- Default Statcounter code for Kaiqing's Website https://kzhang66.github.io/
-->
<script type="text/javascript">
var sc_project=12155422; 
var sc_invisible=1; 
var sc_security="73574776"; 
var sc_https=1; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js" async></script>
<noscript><div class="statcounter"><a title="Web Analytics Made Easy -
StatCounter" href="https://statcounter.com/" target="_blank"><img
class="statcounter" src="https://c.statcounter.com/12155422/0/73574776/1/"
alt="Web Analytics Made Easy - StatCounter"></a></div></noscript>
<!-- End of Statcounter Code -->
</table>
</body>
</html>

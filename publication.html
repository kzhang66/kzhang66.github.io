<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name=viewport content="width=device-width, initial-scale=1">
<meta name="google-site-verification" content="google4c0a25b43370ae9c.html" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="icon" href="photo_kaiqing.png" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Kaiqing Zhang's Homepage</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="misc.html">Misc.</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications</h1>
</div></div>  

<h3>See the more complete and timely updated list at <a href="https://scholar.google.com/citations?hl=en&user=Hi7ZdhQAAAAJ&sortby=pubdate&view_op=list_works&gmla=AJsN-F6J8zj9uxG4y2WQ9nkE5sd25QY7XEP0BqGZbLQNyhK-tewCB_-d8oaGeQ7rXg1J4T5ityD9cXZBtVt46c-c_oJndAKkXEjAciIoZnPaBw3vFvFutgk">Google Scholar</a>.</h3>


<h2>Monographs</h2>
<!-- <ol reversed> -->
<li>
<a href="https://arxiv.org/pdf/2210.04810.pdf">Towards a Theoretical Foundation of Policy Optimization for Learning Control Policies</a></p>
<ul>
<li><p>B. Hu, <b>K. Zhang</b>, N. Li, M. Mesbahi, M. Fazel, T. Başar</p>
</li>
<li><p><i>Annual Review of Control, Robotics, and Autonomous Systems</i>, 2023 (Invited & Refereed).</p>
</li></ul>
</li> 
</ol>
<li>
<a href="https://arxiv.org/pdf/2111.11743.pdf">Independent Learning in Stochastic Games</a></p>
<ul>
<li><p>A. Ozdaglar*, M. O. Sayin*, <b>K. Zhang</b>*</p>
</li>
<li><p><i>International Congress of Mathematicians (ICM)</i>, 2022 (Invited).</p>
</li></ul>
</li> 
</ol>
<li>
<a href="https://arxiv.org/pdf/1911.10635.pdf">Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms</a></p>
<ul>
<li><p><b>K. Zhang</b>, Z. Yang, and T. Başar</p>
</li>
<li><p><i>Springer Studies in Systems, Decision and Control, Handbook on RL and Control</i>, 2020 (Invited).</p>
  </li></ul>
</li> 
</ol>
  
<h2>Journal Papers</h2>
<!-- <ol reversed> --> 
<li><a href="https://arxiv.org/pdf/2308.08705.pdf">Partially observable multi-agent reinforcement learning with information sharing</a></p>
<ul>
<li><p>X. Liu and <b>K. Zhang</b></p>
</li>
<li><p><i>SIAM Journal on Control and Optimization (SICON)</i> (under review) (Short version appeared at <i>ICML 2023</i>).</p>
</li></ul>  
</li>
<li><a href="https://arxiv.org/pdf/2212.13861.pdf">Offline reinforcement learning via linear programming with error-bound induced constraints</a></p>
<ul>
<li><p>A. Ozdaglar*, S. Pattathil*, J. Zhang*, and <b>K. Zhang</b>*</p>
</li>
<li><p><i>Mathematics of Operations Research (MathOR)</i> (under review) (Short version appeared at <i>ICML 2023</i>).</p>
</li></ul>  
</li>
<li><a href="https://arxiv.org/pdf/2409.01447">Last-iterate convergence of payoff-based independent learning in zero-sum stochastic games</a></p>
<ul>
<li><p>Z. Chen, <b>K. Zhang</b>, E. Mazumdar, A. Ozdaglar, A. Wierman</p>
</li>
<li><p><i>Operations Research (OR)</i> (under review) (Short version appeared at <i>NeurIPS 2023</i>).</p>
</li></ul>  
</li>  
<li><a href="https://arxiv.org/pdf/2206.02346">Convergence and sample complexity of natural policy gradient primal-dual methods for constrained MDPs</a></p>
<ul>
<li><p>D. Ding, <b>K. Zhang</b>, T. Başar, and M.R. Jovanovic</p>
</li>
<li><p><i>Journal of Machine Learning Research (JMLR) 2025</i> (Short version appeared at <i>NeurIPS 2020)</i>.</p>   
</li></ul>
</li>    
<li><a href="https://arxiv.org/pdf/1910.09496.pdf">Policy optimization for H-2 linear control with H-infinity robustness guarantee: Implicit regularization and global convergence</a></p>
<ul>
<li><p><b>K. Zhang</b>, B. Hu, and T. Başar</p>
</li>
<li><p><i>SIAM Journal on Control and Optimization (SICON)</i>, 2021 (Short version appeared at <i>L4DC 2020 (<b><i>Oral</i></b>)</i>).</p>   
</li></ul>
<li><a href="https://arxiv.org/pdf/2010.03161.pdf">Model-free non-stationary RL: Near-optimal regret and applications in multi-agent RL and inventory control</a></p>
<ul>
<li><p>W. Mao, <b>K. Zhang</b>, R. Zhu, D. Simchi-Levi, and T. Başar</p>
</li>
<li><p><i>Management Science (MS)</i>, 2023.</p>   
</li></ul>
</li>
<li><a href="https://arxiv.org/pdf/2007.07461.pdf">Model-based multi-agent RL in zero-sum Markov games with near-optimal sample complexity</a></p>
<ul>
<li><p><b>K. Zhang</b>, S.M. Kakade, T. Başar, and L.F. Yang</p>
</li>
<li><p><i>Journal of Machine Learning Research (JMLR) 2023</i> (Short version appeared at <i>NeurIPS 2020 <b>(Spotlight)</b>)</i>.</p>   
</li></ul>
</li>  
<li><a href="https://arxiv.org/pdf/1812.02783.pdf">Finite-sample analysis for decentralized batch multi-agent reinforcement learning with networked agents</a></p>
<ul>
<li><p><b>K. Zhang</b>, Z. Yang, H. Liu, T. Zhang, and T. Başar</p>
</li>
<li><p><i>IEEE Trans. on Automatic Control (TAC)</i>, 2021.</p>   
</li></ul>
</li>
<!-- <li><a href="https://arxiv.org/pdf/1812.03239.pdf">Communication-efficient distributed reinforcement learning</a></p>
<ul>
<li><p>T. Chen, <b>K. Zhang</b>, G. B. Giannakis, and T. Başar</p>
</li></ul>
</li>  -->
<li><a href="https://arxiv.org/pdf/1906.08383.pdf">Global convergence of policy gradient methods to (almost) locally optimal policies</a></p>
<ul>
<li><p><b>K. Zhang</b>, A. Koppel, H. Zhu, and T. Başar</p>
</li>
<li><p><i>SIAM Journal on Control and Optimization (SICON)</i>, 2020.</p>   
</li></ul>
</li>
<li><a href="https://arxiv.org/pdf/1811.07799.pdf">Distributed learning of average belief over networks using sequential observations</a></p>
<ul>
<li><p><b>K. Zhang</b>, Y. Liu, J. Liu, M. Liu, and T. Başar</p>
</li>
<li><p><i>Automatica</i>, 2020.</p>
</li></ul>
</li>
<!-- <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8678800">Projected stochastic primal-dual method for constrained online learning with kernels</a></p>
<ul>
<li><p>A. Koppel*, <b>K. Zhang</b>*, H. Zhu, and T. Başar</p>
</li>
<li><p><i>IEEE Trans. on Signal Processing (TSP)</i>, 2019.</p>
</li></ul>
</li>        -->
<!-- <li><a href="https://reader.elsevier.com/reader/sd/pii/S0968090X18306120?token=29D127CACDBE04A5482EF00C95D09156A632E2182B1E394C28B6A3A81FA293399358AED14423C1345BA5FD56B5C0E3F0">Dynamic operations and
pricing of electric unmanned aerial vehicle systems and power networks</a></p>
<ul>
<li><p><b>K. Zhang</b>, L. Lu, C. Lei, H. Zhu, and Y. Ouyang</p>
</li>
<li><p><i>Journal of Transp. Research Part C: Emerging Technologies</i>, vol. 92, pp. 472-485, July, 2018.</p>
</li></ul>
</li> -->
<!-- <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8360012">Dynamic power distribution system management with a locally connected communication network</a></p>
<ul>
<li><p><b>K. Zhang</b>, W. Shi, H. Zhu, E. Dall’Anese, and T. Başar </p>
</li>
<li><p><i>IEEE Journal of Selected Topics in Signal Processing (JSTSP)</i>, vol. 12, no. 4, pp. 673-687,  May, 2018.</p>
</li></ul>
</li> -->
<!-- <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8328921">Optimal joint bidding and pricing of profit-seeking load serving entity</a></p>
<ul>
<li><p>H. Xu, <b>K. Zhang</b>, and J. Zhang </p>
</li> -->
<!-- <li><p><i>IEEE Trans. on Power Systems (TPS)</i>, vol. 33, no. 5, pp. 5427-5436, March, 2018.</p>
</li></ul>
</li> -->
<!-- <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8012304">Consumption behavior analytics-aided energy forecasting and dispatch</a></p>
<ul> -->
<!-- <li><p>Y. Zhang, R. Yang, <b>K. Zhang</b>, H. Jiang, and J. J. Zhang </p>
</li>
<li><p><i>IEEE Intelligent Systems (IS)</i>, vol. 32, no. 4, pp. 59-63, Aug., 2018.</p>
</li></ul> -->
<!-- </li> -->
<!-- <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7728116">Dependency analysis and improved parameter estimation for complex dynamic load modeling</a></p>
<ul>
<li><p><b>K. Zhang</b>, S. Guo, and H. Zhu, </p>
</li>
<li><p><i>IEEE Trans. on Power Systems (TPS)</i>, vol. 32, no. 4, pp. 3287-3297, Nov. 2016.</p>
</li></ul>
</li> -->
<!-- <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7158270">Enhanced multi-parameter cognitive architecture for future wireless communications</a></p>
<ul>
<li><p>F. Gao, <b>K. Zhang</b>,</p>
</li>
<li><p><i>IEEE Commun. Magazine</i>, vol. 53, no. 7, pp. 86-92, Jul. 2015.</p>
</li></ul> -->
</ol>
  
</li>
</ul>
<h2>Conference Papers</h2> 
<!-- <ol reversed> -->
<li><a href="https://arxiv.org/pdf/2403.16843">Do LLM agents have regret? A case study in online learning and games</a></p>
<ul>
<li><p>C. Park*, X. Liu*, A. Ozdaglar, and <b>K. Zhang</b></p>
</li>
<li><p><i>Intl. Conf. on Learning Represent. (ICLR)</i>, 2025. (Preliminary version <b><i>Oral</i></b> in <i>How Far Are We From AGI Workshop</i>, ICLR 2024)</p>
</li></ul>  
<li><a href="https://arxiv.org/pdf/2412.00985">Provable partially observable reinforcement learning with privileged information</a></p>
<ul>
<li><p>Y. Cai*, X. Liu*, A. Oikonomou*, <b>K. Zhang</b>*</p>
</li>
<li><p><i>Neural Info. Process. Systems (NeurIPS)</i>, 2024.</p>
</li></ul>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3670865.3673491">Two-timescale Q-learning with function approximation in zero-sum stochastic games</a></p>
<ul>
<li><p>Z. Chen, <b>K. Zhang</b>, E. Mazumdar, A. Ozdaglar, A. Wierman</p>
</li>
<li><p><i>ACM Conf. on Econ. and Comput. (EC)</i>, 2024.</p>
</li></ul>  
<li><a href="https://arxiv.org/pdf/2307.09470.pdf">Multi-player zero-sum Markov games with networked separable interactions</a></p>
<ul>
<li><p>C. Park*, <b>K. Zhang</b>*, and A. Ozdaglar</p>
</li>
<li><p><i>Neural Info. Process. Systems (NeurIPS)</i>, 2023.</p>
</li></ul>	
<li><a href="https://arxiv.org/pdf/2204.03991.pdf">The complexity of Markov equilibrium in stochastic games</a></p>
<ul>
<li><p>C. Daskalakis*, N. Golowich*, and <b>K. Zhang*</b></p>
</li>
<li><p><i>Conference on Learning Theory (COLT)</i>, 2023.</p>
</li></ul>	
<li><a href="https://arxiv.org/pdf/2307.06457.pdf">Tackling combinatorial distribution shift: A matrix completion perspective</a></p>
<ul>
<li><p>M. Simchowitz, A. Gupta, and <b>K. Zhang</b> </p>
</li>
<li><p><i>Conference on Learning Theory (COLT)</i>, 2023.</p>
</li></ul>
<li><a href="https://arxiv.org/pdf/2302.03673.pdf">Breaking the curse of multiagents in a large state space: RL in Markov games with independent linear function approximation</a></p>
<ul>
<li><p>Q. Cui, <b>K. Zhang</b>, and S. Du</p>
</li>
<li><p><i>Conference on Learning Theory (COLT)</i>, 2023.</p>
</li></ul>  
<!-- <li><a href="https://arxiv.org/pdf/2308.08705.pdf">Partially observable multi-agent RL with (quasi-)efficiency: The blessing of information sharing</a></p>
<ul>
<li><p>X. Liu and <b>K. Zhang</b></p>
</li> -->
<!-- <li><p><i>Intl. Conf. on Machine Learning (ICML)</i>, 2023.</p> -->
<!-- </li></ul> -->
<!-- <li><a href="https://arxiv.org/pdf/2212.13861.pdf">Revisiting the linear-programming framework for offline RL with general function approximation</a></p>
<ul>
<li><p>A. Ozdaglar*, S. Pattathil*, J. Zhang*, and <b>K. Zhang</b>*</p>
</li>
<li><p><i>Intl. Conf. on Machine Learning (ICML)</i>, 2023.</p>
</li></ul>	 -->
<li><a href="https://arxiv.org/pdf/2212.14511.pdf">Can direct latent model learning solve linear quadratic Gaussian control?</a></p>
<ul>
<li><p>Y. Tian, <b>K. Zhang</b>, R. Tedrake, and S. Sra</p>
</li>
<li><p><i>Learning for Dynamics & Control (L4DC) <b>(Oral)</b></i>, 2023.</p>   
</li></ul>	
<!-- <li><a href="https://arxiv.org/pdf/2206.04502.pdf">What is a good metric to study generalization of minimax learners?</a></p> -->
<!-- <ul> -->
<!-- <li><p>A. Ozdaglar*, S. Pattathil*, J. Zhang*, and <b>K. Zhang</b>*</p> -->
<!-- </li> -->
<!-- <li><p><i>Neural Info. Process. Systems (NeurIPS)</i>, 2022. (Preliminary version <b><i>Oral</i></b> (4 out of all submissions) in <i>New Frontiers in Adversarial Machine Learning Workshop</i>, ICML 2022)</p> -->
<!-- </li></ul>	 -->
<li><a href="https://proceedings.mlr.press/v162/ding22b/ding22b.pdf">Independent policy gradient for large-scale Markov potential games: Sharper rates, function approximation, and game-agnostic convergence</a></p>
<ul>
<li><p>D. Ding*, C. Wei*, <b>K. Zhang</b>*, and M. Jovanović</p>
</li>
<li><p><i>Intl. Conf. on Machine Learning (ICML) <b>(Long Oral)</b></i>, 2022.</p>
</li></ul>
<li><a href="http://proceedings.mlr.press/v139/mao21b/mao21b.pdf">Do differentiable simulators give better policy gradients?</a></p>
<ul>
<li><p>H. T. Suh, M. Simchowitz, <b>K. Zhang</b>, and R. Tedrake</p>
</li>
<li><p><i>Intl. Conf. on Machine Learning (ICML) <b>(Outstanding Paper Award)</b></i>, 2022.</p>
</li></ul>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3490486.3538289">Fictitious play in Markov games with single controller</a></p>
<ul>
<li><p>M. O. Sayin, <b>K. Zhang</b>, and A. Ozdaglar</p>
</li>
<li><p><i>ACM Conf. on Econ. and Comput. (EC)</i>, 2022.</p>
</li></ul>  
<li><a href="https://proceedings.neurips.cc/paper/2021/file/985e9a46e10005356bbaf194249f6856-Paper.pdf">Decentralized Q-Learning in zero-sum Markov games</a></p>
<ul>
<li><p>M. O. Sayin*, <b>K. Zhang</b>*, D. Leslie, T. Başar, and A. Ozdaglar</p>
</li>
<li><p><i>Neural Info. Process. Systems (NeurIPS)</i>, 2021.</p>
</li></ul>
<!-- <li><a href="http://proceedings.mlr.press/v139/mao21b/mao21b.pdf">Near-optimal model-free reinforcement learning in non-stationary episodic MDPs</a></p> -->
<!-- <ul> -->
<!-- <li><p>W. Mao, <b>K. Zhang</b>, R. Zhu, D. Simchi-Levi, and T. Başar</p>
<!-- </li> -->
<!-- <li><p><i>Intl. Conf. on Machine Learning (ICML)</i>, 2021.</p> -->
<!-- </li></ul>	 -->
<!-- <li><a href="https://openreview.net/pdf?id=P6_q1BRxY8Q">Learning safe multi-agent control with decentralized neural barrier certificates</a></p> -->
<!-- <ul> -->
<!-- <li><p>Z. Qin, <b>K. Zhang</b>, Y. Chen, J. Chen, and C. Fan</p> -->
<!-- </li> -->
<!-- <li><p><i>Intl. Conf. on Learning Represent. (ICLR)</i>, 2021.</p> -->
<!-- </li></ul>	 -->
<!-- <li><a href="https://arxiv.org/pdf/2007.07461.pdf">Model-based multi-agent RL in zero-sum Markov games with near-optimal sample complexity</a></p>
<ul>
<li><p><b>K. Zhang</b>, S.M. Kakade, T. Başar, and L.F. Yang</p>
</li>
<li><p><i>Neural Info. Process. Systems (NeurIPS) <b>(Spotlight)</b></i>, 2020 (Long version accepted to <i>JMLR</i>).</p>
</li></ul> -->
<!-- <li><a href="https://proceedings.neurips.cc/paper/2020/file/fb2e203234df6dee15934e448ee88971-Paper.pdf">On the stability and convergence of robust adversarial reinforcement learning: A case study on linear quadratic systems</a></p>
<ul>
<li><p><b>K. Zhang</b>, B. Hu, and T. Başar</p>
</li>
<li><p><i>Neural Info. Process. Systems (NeurIPS)</i>, virtual, 2020.</p>
</li></ul> -->
<!-- <li><a href="https://papers.nips.cc/paper/2020/file/30de24287a6d8f07b37c716ad51623a7-Paper.pdf">POLY-HOOT: Monte-Carlo planning in continuous space MDPs with non-asymptotic analysis</a></p>
<ul>
<li><p>W. Mao, <b>K. Zhang</b>, Q. Xie, and T. Başar</p>
</li>
<li><p><i>Neural Info. Process. Systems (NeurIPS)</i>, virtual, 2020.</p>
</li></ul> -->
<!-- <li><a href="https://papers.nips.cc/paper/2020/file/5f7695debd8cde8db5abcb9f161b49ea-Paper.pdf">Natural policy gradient primal-dual method for constrained Markov decision processes</a></p>
<ul>
<li><p>D. Ding, <b>K. Zhang</b>, T. Başar, and M.R. Jovanovic</p>
</li>
<li><p><i>Neural Info. Process. Systems (NeurIPS)</i>, 2020.</p>
</li></ul>	 -->
<!-- <li><a href="https://arxiv.org/pdf/2004.01098.pdf">Information state embedding in partially observable cooperative multi-agent reinforcement learning</a></p>
<ul>
<li><p>W. Mao, <b>K. Zhang</b>, E. Miehling, and T. Başar</p>
</li>
<li><p><i>IEEE Conf. on Decision and Control (CDC)</i>, Jeju-do, South Korea, 2020.</p>
</li></ul>	 -->
<!-- <li>Finite-sample analyses for decentralized cooperative multi-agent reinforcement learning from batch data</p>
<ul>
<li><p><b>K. Zhang</b>, Z. Yang, H. Liu, T. Zhang, and T. Başar</p>
</li>
<li><p><i>IFAC World Congress</i>, Berlin, Germany, 2020.</p>
</li></ul>  -->  
<li><a href="https://arxiv.org/pdf/1910.09496.pdf">Policy optimization for H-2 linear control with H-infinity robustness guarantee: Implicit regularization and global convergence</a></p>
<ul>
<li><p><b>K. Zhang</b>, B. Hu, and T. Başar</p> 
</li>
<li><p><i>Learning for Dynamics & Control (L4DC) <b>(Oral)</b></i>, 2020.</p>
</li></ul>
<!-- <li>Approximate equilibrium computation for discrete-time linear-quadratic mean-field games</p>
<ul>
<li><p>M. A. Zaman, <b>K. Zhang</b>, E. Miehling, and T. Başar</p>
</li>
<li><p><i>IEEE American Control Conf. (ACC)</i>, Denver, CO, 2020.</p>
</li></ul> -->
<!-- <li><a href="https://arxiv.org/pdf/1906.00729.pdf">Policy optimization provably converges to Nash equilibria in zero-sum linear quadratic games</a></p>
<ul>
<li><p><b>K. Zhang</b>, Z. Yang, and T. Başar</p>
</li>
<li><p><i>Neural Info. Process. Systems (NeurIPS)</i>, Vancouver, Canada, 2019.</p>
</li></ul> -->
<!-- <li><a href="https://arxiv.org/pdf/1911.04220.pdf">Non-cooperative inverse reinforcement learning</a></p>
<ul>
<li><p>X. Zhang, <b>K. Zhang</b>, E. Miehling, and T. Başar</p>
</li>
<li><p><i>Neural Info. Process. Systems (NeurIPS)</i>, Vancouver, Canada, 2019.</p>
</li></ul> -->
<!-- <li>Convergence and iteration complexity of policy gradient method for infinite-horizon reinforcement learning</p>
<ul>
<li><p><b>K. Zhang</b>, A. Koppel, H. Zhu, and T. Başar</p>
</li>
<li><p><i>IEEE Conf. on Decision and Control (CDC)</i>, Nice, France, 2019.</p>
</li></ul> -->
<!-- <li>A communication-efficient multi-agent actor-critic algorithm for distributed reinforcement learning</p>
<ul>
<li><p>Y. Lin, <b>K. Zhang</b>, Z. Yang, Z. Wang, T. Başar, R. Sandhu, and Ji Liu</p>
</li>
<li><p><i>IEEE Conf. on Decision and Control (CDC)</i>, Nice, France, 2019.</p>
</li></ul> -->
<!-- <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8814803">Online planning for decentralized stochastic control with partial history sharing</a></p>
<ul>
<li><p><b>K. Zhang</b>, E. Miehling, and T. Başar</p>
</li>
<li><p><i>IEEE American Control Conf. (ACC)</i>, Philadelphia, PA, 2019.</p>
</li></ul> -->
<!-- <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8678800">Projected stochastic primal-dual method for constrained online learning with kernels</a></p>
<ul>
<li><p><b>K. Zhang</b>, H. Zhu, and T. Başar, and A. Koppel</p>
</li>
<li><p><i>IEEE Conf. on Decision and Control (CDC)</i>, Miami, FL, 2018.</p>
</li></ul> -->
<!-- <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8619581">Networked multi-agent reinforcement learning in continuous spaces</a></p>
<ul>
<li><p><b>K. Zhang</b>, Z. Yang, and T. Başar</p>
</li>
<li><p><i>IEEE Conf. on Decision and Control (CDC)</i>, Miami, FL, 2018.</p>
</li></ul> -->
<!-- <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8619440">A finite sample analysis of the actor-critic algorithm</a></p>
<ul>
<li><p>Z. Yang, <b>K. Zhang</b>, M. Hong, and T. Başar</p>
</li>
<li><p><i>IEEE Conf. on Decision and Control (CDC)</i>, Miami, FL, 2018.</p>
</li></ul> -->
<li><a href="http://proceedings.mlr.press/v80/zhang18n/zhang18n.pdf">Fully decentralized multi-agent reinforcement learning with networked agents</a></p>
<ul>
<li><p><b>K. Zhang</b>, Z. Yang, H. Liu, T. Zhang, and T. Başar</p>
</li>
<li><p><i>Intl. Conf. on Machine Learning (ICML)</i>, 2018.</p>
</li></ul>
<!-- <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8430919">Distributed equilibrium-learning for power network voltage control with a locally connected communication network</a></p>
<ul>
<li><p><b>K. Zhang</b>, W. Shi, H. Zhu, and T. Başar</p>
</li>
<li><p><i>IEEE American Control Conf. (ACC)</i>, Milwaukee, WI, 2018.</p>
</li></ul> -->
<!-- <li><a href="http://proceedings.mlr.press/v84/zhang18a/zhang18a.pdf">Nonlinear structured signal estimation in high dimensions via iterative hard thresholding</a></p>
<ul>
<li><p><b>K. Zhang</b>, Z. Yang, and Z. Wang</p>
</li>
<li><p><i>Intl. Conf. on Artificial Intelligence and Statistics (AISTATS)</i>, Playa Blanca, Lanzarote, Canary Islands, 2018.</p>
</li></ul> -->
<!-- <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8309122">A game theoretic approach for communication-free distribution system management</a></p>
<ul>
<li><p><b>K. Zhang</b> and H. Zhu</p>
</li>
<li><p><i>IEEE Global Conf. on Signal and Info. Process. (GlobalSIP)</i>, Montreal, Canada, 2018.</p>
</li></ul> -->
<!-- <li><a href="https://scholarspace.manoa.hawaii.edu/bitstream/10125/41540/paper0391.pdf">Parameter sensitivity and dependency analysis for the WECC dynamic composite load model</a></p>
<ul>
<li><p><b>K. Zhang</b>, S. Guo, and H. Zhu</p>
</li>
<li><p><i>Hawaii Intl. Conf. System Sciences (HICSS)</i>, Hawaii, 2017.</p>
</li></ul> -->
<!-- <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7511159">On the performance of map-aware cooperative localization</a></p>
<ul>
<li><p><b>K. Zhang</b>, Y. Shen, and M. Z. Win</p>
</li>
<li><p><i>IEEE Intl. Conf. on Commun. (ICC)</i>, 2016.</p>
</li></ul> -->
</ol> 


<!-- </ul> -->
<!-- </div> -->
<div id="footer">
<div id="footer-text">
Page generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
